# Term Deposit Loan Subscription Prediction

## Project Overview

This project implements a machine learning solution to predict customer subscription to term deposit loans. The prediction system analyzes customer data using two primary categories of features:

1. **Background Variables**: Demographic and account-related information about customers
2. **Outreach Variables**: Campaign and communication-related features

The project achieves a recall rate exceeding 84% and accuracy of 87%, providing interpretable decision-making through extensive feature engineering and model optimization.

## Project Structure

The project follows a systematic workflow with multiple notebooks for different stages:

### 1. Exploratory Data Analysis (EDA)

#### Background Variables Analysis
- `exploratory_data_analysis_background_variables_ver2.ipynb`
- `exploratory_data_analysis_background_variables_ver3.ipynb`

These notebooks perform comprehensive exploratory analysis on demographic and account features, including:
- Age group analysis and categorization
- Balance distribution and grouping
- Feature correlation analysis
- Target variable distribution examination

#### Outreach Variables Analysis
- `exploratory_data_analysis_outreach_variables_ver2.ipynb`
- `exploratory_data_analysis_outreach_variables_ver3.ipynb`

These notebooks analyze campaign-related features:
- Contact frequency patterns
- Campaign timing analysis
- Duration impact on subscription
- Feature importance for outreach variables

### 2. Modeling Approaches

The project implements multiple modeling strategies:

#### Approach 1: Background Variables Only
- `model_background_variables__1_.ipynb`

This notebook develops a prediction model using only demographic and account information, serving as a baseline approach.

#### Approach 2: All Features Combined
- `model_all_features-new-features.ipynb`
- `model_all_features_extra_different_weights.ipynb`

These notebooks implement comprehensive models using both background and outreach variables:
- Feature engineering from both variable sets
- Model optimization with different class weights
- Hyperparameter tuning
- Performance evaluation and comparison

## Key Features

### Feature Engineering
- **Age Grouping**: `<30`, `30-60`, `>60`
- **Balance Categorization**: `<1000`, `1000-2000`, `2000-4000`, `>4000`
- **Day Categorization**: Based on campaign timing
- **Derived Features**: Created from interaction and transformation of original variables

### Machine Learning Techniques

#### Data Balancing
- SMOTE (Synthetic Minority Over-sampling Technique)
- RandomOverSampler
- RandomUnderSampler
- Class weight optimization

#### Models Implemented
- **XGBoost Classifier**: Primary model with hyperparameter tuning
- **XGBoost Regressor**: Alternative approach for comparison

#### Model Optimization
- 5-fold cross-validation
- RandomizedSearchCV for hyperparameter tuning
- Class weight adjustment for imbalanced data

#### Evaluation Metrics
- Recall score (primary metric: >84%)
- Accuracy (>87%)
- Confusion matrix
- Classification report
- ROC-AUC score

#### Interpretability
- Feature importance analysis
- Partial Dependence Plots (PDP)
- SHAP-style feature analysis

## Data Requirements

The project expects the following CSV files:

1. **Original Data**:
   - `term-deposit-marketing-2020.csv` - Raw customer and campaign data

2. **Processed Data** (generated by EDA notebooks):
   - `term-deposit-marketing-background-variables.csv`
   - `term-deposit-marketing-background-variables-new-features.csv`
   - `term-deposit-marketing-after-outreach-variables.csv`
   - `term-deposit-marketing-after-outreach-variables-new-features.csv`

### Expected Data Columns

**Background Variables**:
- `age`: Customer age
- `job`: Type of employment
- `balance`: Account balance
- Additional demographic features

**Outreach Variables**:
- `contact`: Contact communication type
- `day`: Day of the month last contacted
- `month`: Month of last contact
- `duration`: Last contact duration in seconds
- `campaign`: Number of contacts during this campaign

**Target Variable**:
- `y`: Has the client subscribed to a term deposit? (binary: yes/no)

## Installation and Setup

### Prerequisites
- Python 3.7 or higher
- Jupyter Notebook or JupyterLab

### Required Libraries

Install all dependencies using pip:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost imbalanced-learn joblib
```

Or use the requirements file:

```bash
pip install -r requirements.txt
```

#### Core Libraries:
- **pandas** (>=1.3.0): Data manipulation and analysis
- **numpy** (>=1.20.0): Numerical computing
- **matplotlib** (>=3.4.0): Data visualization
- **seaborn** (>=0.11.0): Statistical data visualization

#### Machine Learning:
- **scikit-learn** (>=0.24.0): Machine learning algorithms and tools
- **xgboost** (>=1.4.0): Gradient boosting framework
- **imbalanced-learn** (>=0.8.0): Handling imbalanced datasets

#### Utilities:
- **joblib** (>=1.0.0): Model serialization

## Usage

### Running the Notebooks

Follow this recommended sequence for optimal results:

#### Step 1: Exploratory Data Analysis

Start with background variables:
```bash
jupyter notebook exploratory_data_analysis_background_variables_ver3.ipynb
```

Then analyze outreach variables:
```bash
jupyter notebook exploratory_data_analysis_outreach_variables_ver3.ipynb
```

#### Step 2: Model Training

**Option A - Background Variables Only:**
```bash
jupyter notebook model_background_variables__1_.ipynb
```

**Option B - All Features (Recommended):**
```bash
jupyter notebook model_all_features-new-features.ipynb
```

**Option C - All Features with Weight Optimization:**
```bash
jupyter notebook model_all_features_extra_different_weights.ipynb
```

### Workflow

1. **Data Loading**: Load the raw CSV data
2. **Feature Engineering**: Create derived features based on EDA insights
3. **Data Splitting**: Split into training (80%) and testing (20%) sets
4. **Data Balancing**: Apply SMOTE or class weights to handle imbalanced data
5. **Model Training**: Train XGBoost classifier with optimized parameters
6. **Evaluation**: Assess model performance using recall, accuracy, and confusion matrix
7. **Interpretation**: Analyze feature importance and partial dependence plots
8. **Model Saving**: Save trained model using joblib for future use

## Model Performance

### Best Results
- **Recall**: >84% (primary optimization target)
- **Accuracy**: >87%
- **Cross-validation**: 5-fold validation implemented

### Key Insights
- Outreach variables significantly improve prediction accuracy
- Balance and age groups are important demographic predictors
- Campaign duration and contact frequency are critical outreach factors
- Model provides interpretable results through feature importance analysis

## Customization

### Adjusting Class Weights
In the model notebooks, modify the `scale_pos_weight` parameter:
```python
model = XGBClassifier(scale_pos_weight=weight_ratio)
```

### Hyperparameter Tuning
Adjust the parameter grid in RandomizedSearchCV:
```python
param_grid = {
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300, 400, 500],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
}
```

### Feature Selection
Remove or add features based on your EDA findings in the preprocessing cells.

## Output Files

The notebooks generate:
- Processed CSV files with engineered features
- Trained model files (`.pkl` format via joblib)
- Visualization plots (feature importance, confusion matrices, PDPs)
- Performance metrics reports

## Troubleshooting

### Common Issues

**Issue**: `FileNotFoundError` for CSV files
- **Solution**: Ensure all required CSV files are in the same directory as the notebooks

**Issue**: Imbalanced data warnings
- **Solution**: The notebooks already implement SMOTE and class weights; adjust parameters if needed

**Issue**: Low recall scores
- **Solution**: Try adjusting `scale_pos_weight` or implementing different balancing techniques

**Issue**: Memory errors
- **Solution**: Reduce dataset size or use batch processing for large datasets

## Contributing

When adding new features or models:
1. Follow the naming convention: `[stage]_[variables]_ver[n].ipynb`
2. Document all feature engineering steps
3. Include evaluation metrics and interpretation
4. Update this README with new approaches


## Contact

For questions or issues, please contact:
- **Email**: schakr18@umd.edu
- **GitHub**: [Your GitHub profile]

## Acknowledgments

This project was developed as part of AI Residency at Apziva, focusing on customer subscription prediction using machine learning and NLP techniques.

---

**Version**: 3.0  
**Last Updated**: November 2025
