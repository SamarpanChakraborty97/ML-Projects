# ğŸ¦ Term Deposit Loan Subscription Prediction

A comprehensive machine learning project to predict customer subscription to term deposit loans using advanced feature engineering, multiple modeling approaches, and interpretable ML techniques. This project achieves 87% accuracy and 84%+ recall through systematic analysis of demographic and campaign data.

## ğŸ“‹ Project Overview

This project implements a robust machine learning solution to predict customer subscription to term deposit loans. The prediction system analyzes customer data using two primary categories of features:

1. **Background Variables**: Demographic and account-related information about customers
2. **Outreach Variables**: Campaign and communication-related features

The project achieves a **recall rate exceeding 84%** and **accuracy of 87%**, providing interpretable decision-making through extensive feature engineering and model optimization.

### âœ¨ Key Features

- **Two-Phase Feature Engineering**: Separate analysis of background and outreach variables
- **Multiple Modeling Approaches**: Baseline, comprehensive, and weight-optimized models
- **Imbalanced Data Handling**: SMOTE and class weight optimization techniques
- **High Performance**: 87% accuracy and 84%+ recall rate
- **Model Interpretability**: Feature importance analysis and partial dependency plots
- **Production Ready**: Saved models with complete preprocessing pipeline

## ğŸ“ Project Structure

The project follows a systematic workflow with multiple notebooks for different stages:

### 1. ğŸ” Exploratory Data Analysis (EDA)

#### ğŸ“Š Background Variables Analysis
- `exploratory_data_analysis_background_variables_ver2.ipynb`
- `exploratory_data_analysis_background_variables_ver3.ipynb`

These notebooks perform comprehensive exploratory analysis on demographic and account features, including:
- Age group analysis and categorization
- Balance distribution and grouping
- Feature correlation analysis
- Target variable distribution examination

#### ğŸ“ Outreach Variables Analysis
- `exploratory_data_analysis_outreach_variables_ver2.ipynb`
- `exploratory_data_analysis_outreach_variables_ver3.ipynb`

These notebooks analyze campaign-related features:
- Contact frequency patterns
- Campaign timing analysis
- Duration impact on subscription
- Feature importance for outreach variables

### 2. ğŸ¤– Modeling Approaches

The project implements multiple modeling strategies:

#### ğŸ¯ Approach 1: Background Variables Only
- `model_background_variables__1_.ipynb`

This notebook develops a prediction model using only demographic and account information, serving as a baseline approach.

#### ğŸ“ Approach 2: All Features Combined
- `model_all_features-new-features.ipynb`
- `model_all_features_extra_different_weights.ipynb`

These notebooks implement comprehensive models using both background and outreach variables:
- Feature engineering from both variable sets
- Model optimization with different class weights
- Hyperparameter tuning
- Performance evaluation and comparison

## ğŸ¯ Key Features

### ğŸ”¨ Feature Engineering
- **Age Grouping**: `<30`, `30-60`, `>60`
- **Balance Categorization**: `<1000`, `1000-2000`, `2000-4000`, `>4000`
- **Day Categorization**: Based on campaign timing
- **Derived Features**: Created from interaction and transformation of original variables

### ğŸ§  Machine Learning Techniques

#### âš–ï¸ Data Balancing
- SMOTE (Synthetic Minority Over-sampling Technique)
- RandomOverSampler
- RandomUnderSampler
- Class weight optimization

#### ğŸ“¦ Models Implemented
- **XGBoost Classifier**: Primary model with hyperparameter tuning
- **XGBoost Regressor**: Alternative approach for comparison

#### âš™ï¸ Model Optimization
- 5-fold cross-validation
- RandomizedSearchCV for hyperparameter tuning
- Class weight adjustment for imbalanced data

#### ğŸ“ Evaluation Metrics
- Recall score (primary metric: >84%)
- Accuracy (>87%)
- Confusion matrix
- Classification report
- ROC-AUC score

#### ğŸ” Interpretability
- Feature importance analysis
- Partial Dependence Plots (PDP)
- SHAP-style feature analysis

## ğŸ“Š Data Requirements

The project expects the following CSV files:

1. **Original Data**:
   - `term-deposit-marketing-2020.csv` - Raw customer and campaign data

2. **Processed Data** (generated by EDA notebooks):
   - `term-deposit-marketing-background-variables.csv`
   - `term-deposit-marketing-background-variables-new-features.csv`
   - `term-deposit-marketing-after-outreach-variables.csv`
   - `term-deposit-marketing-after-outreach-variables-new-features.csv`

### Expected Data Columns

**Background Variables**:
- `age`: Customer age
- `job`: Type of employment
- `balance`: Account balance
- Additional demographic features

**Outreach Variables**:
- `contact`: Contact communication type
- `day`: Day of the month last contacted
- `month`: Month of last contact
- `duration`: Last contact duration in seconds
- `campaign`: Number of contacts during this campaign

**Target Variable**:
- `y`: Has the client subscribed to a term deposit? (binary: yes/no)

## ğŸš€ Installation and Setup

### Prerequisites
- Python 3.7 or higher
- Jupyter Notebook or JupyterLab

### Required Libraries

Install all dependencies using pip:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost imbalanced-learn joblib
```

Or use the requirements file:

```bash
pip install -r requirements.txt
```

#### Core Libraries:
- **pandas** (>=1.3.0): Data manipulation and analysis
- **numpy** (>=1.20.0): Numerical computing
- **matplotlib** (>=3.4.0): Data visualization
- **seaborn** (>=0.11.0): Statistical data visualization

#### Machine Learning:
- **scikit-learn** (>=0.24.0): Machine learning algorithms and tools
- **xgboost** (>=1.4.0): Gradient boosting framework
- **imbalanced-learn** (>=0.8.0): Handling imbalanced datasets

#### Utilities:
- **joblib** (>=1.0.0): Model serialization

## ğŸ’» Usage

### Running the Notebooks

Follow this recommended sequence for optimal results:

#### Step 1: Exploratory Data Analysis

Start with background variables:
```bash
jupyter notebook exploratory_data_analysis_background_variables_ver3.ipynb
```

Then analyze outreach variables:
```bash
jupyter notebook exploratory_data_analysis_outreach_variables_ver3.ipynb
```

#### Step 2: Model Training

**Option A - Background Variables Only:**
```bash
jupyter notebook model_background_variables__1_.ipynb
```

**Option B - All Features (Recommended):**
```bash
jupyter notebook model_all_features-new-features.ipynb
```

**Option C - All Features with Weight Optimization:**
```bash
jupyter notebook model_all_features_extra_different_weights.ipynb
```

### ğŸ”„ Workflow

1. **Data Loading**: Load the raw CSV data
2. **Feature Engineering**: Create derived features based on EDA insights
3. **Data Splitting**: Split into training (80%) and testing (20%) sets
4. **Data Balancing**: Apply SMOTE or class weights to handle imbalanced data
5. **Model Training**: Train XGBoost classifier with optimized parameters
6. **Evaluation**: Assess model performance using recall, accuracy, and confusion matrix
7. **Interpretation**: Analyze feature importance and partial dependence plots
8. **Model Saving**: Save trained model using joblib for future use

## ğŸ“ˆ Model Performance

### Best Results
- **Recall**: >84% (primary optimization target)
- **Accuracy**: >87%
- **Cross-validation**: 5-fold validation implemented

### ğŸ’¡ Key Insights
- ğŸ“ Outreach variables significantly improve prediction accuracy
- ğŸ‘¥ Balance and age groups are important demographic predictors
- â±ï¸ Campaign duration and contact frequency are critical outreach factors
- ğŸ“Š Model provides interpretable results through feature importance analysis

## ğŸ› ï¸ Customization

### Adjusting Class Weights
In the model notebooks, modify the `scale_pos_weight` parameter:
```python
model = XGBClassifier(scale_pos_weight=weight_ratio)
```

### Hyperparameter Tuning
Adjust the parameter grid in RandomizedSearchCV:
```python
param_grid = {
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300, 400, 500],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
}
```

### Feature Selection
Remove or add features based on your EDA findings in the preprocessing cells.

## ğŸ’¾ Output Files

The notebooks generate:
- Processed CSV files with engineered features
- Trained model files (`.pkl` format via joblib)
- Visualization plots (feature importance, confusion matrices, PDPs)
- Performance metrics reports

## ğŸ”§ Troubleshooting

### Common Issues

**Issue**: `FileNotFoundError` for CSV files
- **Solution**: Ensure all required CSV files are in the same directory as the notebooks

**Issue**: Imbalanced data warnings
- **Solution**: The notebooks already implement SMOTE and class weights; adjust parameters if needed

**Issue**: Low recall scores
- **Solution**: Try adjusting `scale_pos_weight` or implementing different balancing techniques

**Issue**: Memory errors
- **Solution**: Reduce dataset size or use batch processing for large datasets

## ğŸ¤ Contributing

When adding new features or models:
1. Follow the naming convention: `[stage]_[variables]_ver[n].ipynb`
2. Document all feature engineering steps
3. Include evaluation metrics and interpretation
4. Update this README with new approaches

## ğŸ“„ License

[Specify your license here]

## ğŸ“§ Contact

For questions or issues, please contact:
- **Email**: schakr18@umd.edu
- **LinkedIn**: [linkedin.com/in/samarpan-chakraborty](https://linkedin.com/in/samarpan-chakraborty)
- **GitHub**: [github.com/SamarpanChakraborty97](https://github.com/SamarpanChakraborty97)

## ğŸ™ Acknowledgments

This project was developed as part of AI Residency at Apziva, focusing on automating HR talent acquisition and customer subscription prediction using machine learning and NLP techniques.

---

**Note**: This project demonstrates practical application of machine learning for financial services and marketing analytics. The interpretable models provide actionable insights for improving customer targeting and campaign optimization in term deposit loan marketing.

**Version**: 3.0  
**Last Updated**: November 2025
